% !TEX encoding = UTF-8 Unicode
\documentclass[a4paper,12pt]{article}
	
	% В этом документе преамбула
	
	%%% Работа с русским языком
	\usepackage{cmap}					% поиск в PDF
	\usepackage{mathtext} 				% русские буквы в формулах
	\usepackage[T2A]{fontenc}			% кодировка
	\usepackage[utf8]{inputenc}			% кодировка исходного текста
	\usepackage[english]{babel}	% локализация и переносы
	\usepackage{indentfirst}
	\frenchspacing
	
	\renewcommand{\epsilon}{\ensuremath{\varepsilon}}
	\renewcommand{\phi}{\ensuremath{\varphi}}
	\renewcommand{\kappa}{\ensuremath{\varkappa}}
	\renewcommand{\le}{\ensuremath{\leqslant}}
	\renewcommand{\leq}{\ensuremath{\leqslant}}
	\renewcommand{\ge}{\ensuremath{\geqslant}}
	\renewcommand{\geq}{\ensuremath{\geqslant}}
	\renewcommand{\emptyset}{\varnothing}
	
	%%% Дополнительная работа с математикой
	\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
	\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
	
	%% Номера формул
	%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
	%\usepackage{leqno} % Нумереация формул слева
	
	%% Свои команды
	\DeclareMathOperator{\sgn}{\mathop{sgn}}
	
	%% Перенос знаков в формулах (по Львовскому)
	\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
		{\hbox{$\mathsurround=0pt #1$}}{}}
	
	%%% Работа с картинками
	\usepackage{graphicx}  % Для вставки рисунков
	\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
	\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
	\usepackage{wrapfig} % Обтекание рисунков текстом
	
	%%% Работа с таблицами
	\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
	\usepackage{longtable}  % Длинные таблицы
	\usepackage{multirow} % Слияние строк в таблице
	
	%%% Теоремы
	\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять.
	\newtheorem{theorem}{Theorem}
	\newenvironment{Proof} % имя окружения
	{\par\noindent{\bf Proof.}} % команды для \begin
	{\hfill$\scriptstyle\blacksquare$} % команды для \end
	
	\newtheorem{proposition}[theorem]{Утверждение}
	
	\theoremstyle{definition} % "Определение"
	\newtheorem{corollary}{Следствие}[theorem]
	\newtheorem{problem}{Задача}[section]
	
	\theoremstyle{remark} % "Примечание"
	\newtheorem*{nonum}{Решение}
	
	%%% Программирование
	\usepackage{etoolbox} % логические операторы
	
	%%% Страница
	\usepackage{extsizes} % Возможность сделать 14-й шрифт
	\usepackage{geometry} % Простой способ задавать поля
	\geometry{top=25mm}
	\geometry{bottom=35mm}
	\geometry{left=35mm}
	\geometry{right=20mm}
	%
	%\usepackage{fancyhdr} % Колонтитулы
	% 	\pagestyle{fancy}
	%\renewcommand{\headrulewidth}{0pt}  % Толщина линейки, отчеркивающей верхний колонтитул
	% 	\lfoot{Нижний левый}
	% 	\rfoot{Нижний правый}
	% 	\rhead{Верхний правый}
	% 	\chead{Верхний в центре}
	% 	\lhead{Верхний левый}
	%	\cfoot{Нижний в центре} % По умолчанию здесь номер страницы
	
	\usepackage{setspace} % �?нтерлиньяж
	%\onehalfspacing % �?нтерлиньяж 1.5
	%\doublespacing % �?нтерлиньяж 2
	%\singlespacing % �?нтерлиньяж 1
	
	\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
	
	\usepackage{soul} % Модификаторы начертания
	
	\usepackage{hyperref}
	\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}
	\hypersetup{				% Гиперссылки
		unicode=true,           % русские буквы в раздела PDF
		pdftitle={Заголовок},   % Заголовок
		pdfauthor={Автор},      % Автор
		pdfsubject={Тема},      % Тема
		pdfcreator={Создатель}, % Создатель
		pdfproducer={Производитель}, % Производитель
		pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова
		colorlinks=true,       	% false: ссылки в рамках; true: цветные ссылки
		linkcolor=red,          % внутренние ссылки
		citecolor=black,        % на библиографию
		filecolor=magenta,      % на файлы
		urlcolor=blue           % на URL
	}
	
	\usepackage{csquotes} % Еще инструменты для ссылок
	
	%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex}
	
	\usepackage{multicol} % Несколько колонок
	
	\usepackage{tikz} % Работа с графикой
	\usepackage{pgfplots}
	\usepackage{pgfplotstable}
	
	\usepackage{algorithm}
	\usepackage[noend]{algcompatible}
	
	\newcommand{\ba}{\mathbf{a}}
	\newcommand{\bb}{\mathbf{b}}
	\newcommand{\bw}{\mathbf{w}}
	\newcommand{\bQ}{\mathbf{Q}}
	\newcommand{\bW}{\mathbf{W}}
	\newcommand{\by}{\mathbf{y}}
	\newcommand{\bx}{\mathbf{x}}
	\newcommand{\bz}{\mathbf{z}}
	\newcommand{\bY}{\mathbf{Y}}
	\newcommand{\bX}{\mathbf{X}}
	\newcommand{\cA}{\mathcal{A}}
	\newcommand{\cJ}{\mathcal{J}}
	\newcommand{\bbR}{\mathbb{R}}
	
	\newcommand{\T}{{\text{\tiny\sffamily\upshape\mdseries T}}}
	\newcommand{\argmin}{\mathop{\arg \min}\limits}
	\newcommand{\argmax}{\mathop{\arg \max}\limits}
	
	\begin{document}
		
	\section*{Problem Statement}
	We consider the problem of predicting an target variable $y$ given an $n$-dimensional object $\bx$. 
	The target variable $y$ is assumed to be continuous for regression task and binary for classification task.
	The goal is to build a model $f$ which outcomes a prediction for each input object.
	We assume that the function~$f (\bx | \bw)$ is a feed-forward dense neural network with 1 hidden, and 1 output layer. The hidden layer dimension is denoted by $h$. The model output is
	\[
	f(\bx | \bw) = \sigma_2\left(\bW_2 \sigma_1(\bW_1 \bx)\right),
	\]
	where $\bW_1 \in \bbR^{h \times n}$, $\bW_2 \in \bbR^{1 \times h}$ are weight matrices, $\sigma_1$, $\sigma_2$ are activation functions applied to each input component. We omitted the bias terms for simplicity.
	Let denote by $\bw = (\text{vec}(\bW_1), \text{vec}(\bW_2)) \in \bbR^p$ a joint weight vector, where $p=h(n + 1)$.
	
	Let assume that we are given the design matrix~$\bX = [\bx_1, \dots, \bx_m]^{\T} \in \bbR^{m \times n}$ and the target vector~$\by = [y_1, \dots, y_m]^{\T} \in \bbR^{m}$. 
	Each $i$-th matrix $\bX$ row represents an object which is associated with the $i$-th vector~$\by$ element.
	The goal is to find the optimal weight vector $\bw^*$.
	
	The weights~$\bw$ are fitted by the minimization of an error function
	\begin{equation}
	\bw^* = \argmin_{\bw \in \bbR^r} S(\bw | \bX, \by, f).
	\label{eq:error_function}
	\end{equation}
	The most common choices for the error function~$S(\bw | \bX, \by, f)$ are
	\begin{itemize}
		\item squared error for regression task: 
		$$
			S(\bw | \bX, \by, f) = \sum_{i=1}^m \| y_i - f(\bx_i | \bw)\|^2;
		$$
		\item cross-entropy for classification task: 
		$$
			S(\bw | \bX, \by, f) = \sum_{i=1}^m \left[y_i \log f (\bx_i | \bw) + (1-y_i) \log (1 - f (\bx_i | \bw))\right].
		$$
	\end{itemize}
	The problem~\ref{eq:error_function} could be solved by one of the neural network optimization methods~[].
	
	The number of model weights~$p$ could be extremely huge. 
	In this case the solution of the problem~\ref{eq:error_function} leads to overfitting. 
	To eliminate this problem we propose to select the subset~$\cA \subseteq \{1, \dots, p\}$ of the active weights. 
	The weights which are not active are supposed to be zero.
	To choose the subset~$\cA$ from all possible $2^p$ combinations let introduce a quality criteria~$Q(\cA | \bX, \by, f)$. 
	This function evaluates the quality of a particular active set $\cA$. 
	We desire to find the optimal subset $\cA^*$ which minimize the function
	\begin{equation}
		\cA^* = \argmin_{\cA \subseteq \{1, \dots, p\}} Q(\cA | \bX, \by, f).
		\label{eq:quality_criteria}
	\end{equation}
	
	If the solution $\cA^*$ of the~\ref{eq:quality_criteria} is given the next step is to determine the optimal model weights	$\bw^*$ by solving a problem
	
	\begin{equation}
		\bw^* = \argmin_{\bw \in \bbR^r} S(\bw | \bX, \by, f), \quad \text{subject to } w_j = 0 \text{ for } j \notin \cA^*.
		\label{eq:reduced_error_function}
	\end{equation}
	
	\section*{Feature selection}
	
	To find the optimal subset $\cA^*$ we suggest to use the approach similar to the mRMR method for feature selection problem. 
	This method tries to select features which have the minimal redundancy and maximum relevance.
	To formalise this approach let introduce two functions: Sim and Rel. 
	The former measures the redundancy between features, the latter contains relevances between each feature and target vector. 
	We want to minimize the Sim function and maximize the Rel simultaneously.
	The mRMR method uses the difference of these functions
	$\text{Sim} -\text{Rel}$ to choose the optimal feature subset.
	
	The QPFS method offers the explicit way to construct the functions Sim and Rel. 
	The method minimize the following functional
	\begin{equation}
		(1 - \alpha) \bz^{\T} \bQ \bz - \alpha \mathbf{b}^{\T} \bz \rightarrow \min_{\substack{\bz \in \bbR^n_+ \\ \|\bz\|_1 = 1}}.
		\label{eq:quadratic_problem}
	\end{equation}
	This functional is an analogue of the described quality criteria~\ref{eq:quality_criteria}.
  	The first term is associated with the Sim function and the second with the Rel. 
  	The matrix $\bQ \in \bbR^{n \times n}$ entries measures the pairwise similarities between features. 
  	The vector $\mathbf{b} \in \bbR^n$ express the similarities between each feature and the target vector.
  	The normalized vector~$\bz$ shows the importance of each feature. 
  	This approach penalizes the dependent features and encourages features relevant to the target. 
  	The parameter $\alpha$ allows to control the impact of the Sim and the Rel terms.
  	To find the optimal feature subset the thresholding for $\bz$ could be applied.
  	
  	To measure similarity it was proposed to use the absolute value of sample Pearson correlation coefficient or sample mutual information.
  	The problem~\ref{eq:quadratic_problem} is convex if the matrix $\bQ$ is positive semidefinite. In general it is not always true. To satisfy this condition one could replace this matrix by $\bQ - \lambda_{\text{min}} \mathbf{I}$, where $\lambda_{\text{min}} $ is a minimal eigenvalue of~$\bQ$.
  	  	
  	\section*{Model weights selection}
  	
	We propose the algorithm that extends the quadratic programming methodology to the case of model weights selection.
	Let suppose that the weights from different layers do not interact. 
	This assumption allows to solve the model weights selection problem separately for each layer.
	We introduce two vectors $\bz_1 \in \bbR^{hn}_{+}, \|\bz_1\|_1 = 1$ and $\bz_2 \in \bbR^{rh}_{+}, \|\bz_2\|_1 = 1$ for matrices $\bW_1$ and $\bW_2$ respectively. 
	The components of these vectors express the importance of each individual weight.
	Let introduce the soft version of the problem~\ref{eq:quality_criteria}
	\begin{align*}
		\bz_1^* = \argmin_{\substack{\bz_1 \in \bbR^{hn}_{+} \| \bz_1 \|_1 = 1}} Q(\bz_1 | \bX, \by, f) \\
		\bz_2^* = \argmin_{\substack{ \bz_2 \in \bbR^{h}_{+} \| \bz_2 \|_1 = 1}} Q(\bz_2 | \bX, \by, f) .
		\label{eq:soft_quality_criteria}
	\end{align*}

	Since the vectors $\bz_1$ and $\bz_2$ are obtained, we have to recover the optimal active subset $\cA^*$. 
	Thresholds $d_1$ and $d_2$ are tuned for both layers. If the weight importance larger than corresponding threshold the weight is assumed to be active.
	
	Similarly to the authors of QPFS we use the quadratic quality criteria for each layer
	\begin{equation}
		Q(\bz | \bX, \by, f) = (1 - \alpha) \bz^{\T} \bQ\bz - \alpha \mathbf{b}^{\T} \bz.
	\end{equation}
	
	The matrix $\bQ$ entries estimate the pairwise interactions between weights. The vector $\bb$ estimates the influence of each weight to the target variable.
	
	The interactions between weights measured by calculating the similarity function (correlation or mutual information) ...
	
	\hrulefill
	
	The vector $\bb$ entries estimate the influence of the weights to the target variable. Let approximate the function $f(\bx | \bw)$ at the point $\bw$ by its linearization
	\begin{equation}
		\mathbf{f} (\bX | \bw + \Delta \bw) \approx \mathbf{f}(\bX | \bw) + \mathbf{J} \cdot \Delta \bw,
	\end{equation}
	where $\mathbf{J} \in \bbR^{m \times r}$ is a Jacobian matrix
	\begin{equation}
		\mathbf{J} = 
		\begin{pmatrix}
			\frac{\partial f(\bx_1 | \bw)}{\partial w_1} & \dots & 
			\frac{\partial f(\bx_1 | \bw)}{\partial w_r} \\
			\dots & \dots & \dots \\
			\frac{\partial f(\bx_m | \bw)}{\partial w_1} & \dots & 
			\frac{\partial f(\bx_m | \bw)}{\partial w_r}
		\end{pmatrix}.
	\end{equation}
	The $j$-th element of the vector $\bb$ equals the similarity function between the $j$-th column of the matrix $\mathbf{J}$ and the target vector $\by$.
	
	\begin{algorithm}[h]
		\caption{}
		\begin{algorithmic}[1]
			\REQUIRE $\bX, \by$;
			\ENSURE $\ba, \bw$;
			\STATE $\ba^0 = (1, \dots, 1)^{\T}$
			\STATE $\bw^0 = \argmin_{\bw \in \bbR^r} S(\bw | \bX, \by, f)$
			\FOR{$k=0,\dots, K$}
			\STATE $\ba^{k+1} = \argmin_{\ba \in \{0,1\}^r} \ba^{\T} \bQ(\bw^k) \ba - \bb^{\T}(\bw^k) \ba, \quad \text{subject to } \ba_{k+1} \odot (1 - \ba_k) = 0$
			\STATE $\bw^{k+1} = \argmin_{\bw \in \bbR^r} S(\bw | \bX, \by, f), \quad \text{subject to } \bw \odot (1 - \ba_{k + 1}) = 0$
			\ENDFOR
		\end{algorithmic}
	\end{algorithm}

	
	\section{Thoughts}
	\begin{itemize}
		\item Постановка -> квадратичный критерий -> в случае линейной регрессии ... -> пусть Q, b зависят от параметров -> предлагаем итеративный алгоритм 
		\item Итеративный алгоритм: либо начиная с первого слоя и дальше, а какой первый шаг?
		\item может столбец матрицы Якоби должен коррелировать с ближайшим нейроном а не выходом
		
	\end{itemize}
	
	\end{document}