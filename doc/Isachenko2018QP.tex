
% !TEX encoding = UTF-8 Unicode
\documentclass[a4paper,12pt]{article}
	
	% В этом документе преамбула
	
	%%% Работа с русским языком
	\usepackage{cmap}					% поиск в PDF
	\usepackage{mathtext} 				% русские буквы в формулах
	\usepackage[T2A]{fontenc}			% кодировка
	\usepackage[utf8]{inputenc}			% кодировка исходного текста
	\usepackage[english]{babel}	% локализация и переносы
	\usepackage{indentfirst}
	\frenchspacing
	
	\renewcommand{\epsilon}{\ensuremath{\varepsilon}}
	\renewcommand{\phi}{\ensuremath{\varphi}}
	\renewcommand{\kappa}{\ensuremath{\varkappa}}
	\renewcommand{\le}{\ensuremath{\leqslant}}
	\renewcommand{\leq}{\ensuremath{\leqslant}}
	\renewcommand{\ge}{\ensuremath{\geqslant}}
	\renewcommand{\geq}{\ensuremath{\geqslant}}
	\renewcommand{\emptyset}{\varnothing}
	
	%%% Дополнительная работа с математикой
	\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
	\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление
	
	%% Номера формул
	%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.
	%\usepackage{leqno} % Нумереация формул слева
	
	%% Свои команды
	\DeclareMathOperator{\sgn}{\mathop{sgn}}
	
	%% Перенос знаков в формулах (по Львовскому)
	\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
		{\hbox{$\mathsurround=0pt #1$}}{}}
	
	%%% Работа с картинками
	\usepackage{graphicx}  % Для вставки рисунков
	\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
	\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
	\usepackage{wrapfig} % Обтекание рисунков текстом
	
	%%% Работа с таблицами
	\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
	\usepackage{longtable}  % Длинные таблицы
	\usepackage{multirow} % Слияние строк в таблице
	
	%%% Теоремы
	\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять.
	\newtheorem{theorem}{Theorem}
	\newenvironment{Proof} % имя окружения
	{\par\noindent{\bf Proof.}} % команды для \begin
	{\hfill$\scriptstyle\blacksquare$} % команды для \end
	
	\newtheorem{proposition}[theorem]{Утверждение}
	
	\theoremstyle{definition} % "Определение"
	\newtheorem{corollary}{Следствие}[theorem]
	\newtheorem{problem}{Задача}[section]
	
	\theoremstyle{remark} % "Примечание"
	\newtheorem*{nonum}{Решение}
	
	%%% Программирование
	\usepackage{etoolbox} % логические операторы
	
	%%% Страница
	\usepackage{extsizes} % Возможность сделать 14-й шрифт
	\usepackage{geometry} % Простой способ задавать поля
	\geometry{top=25mm}
	\geometry{bottom=35mm}
	\geometry{left=35mm}
	\geometry{right=20mm}
	%
	%\usepackage{fancyhdr} % Колонтитулы
	% 	\pagestyle{fancy}
	%\renewcommand{\headrulewidth}{0pt}  % Толщина линейки, отчеркивающей верхний колонтитул
	% 	\lfoot{Нижний левый}
	% 	\rfoot{Нижний правый}
	% 	\rhead{Верхний правый}
	% 	\chead{Верхний в центре}
	% 	\lhead{Верхний левый}
	%	\cfoot{Нижний в центре} % По умолчанию здесь номер страницы
	
	\usepackage{setspace} % �?нтерлиньяж
	%\onehalfspacing % �?нтерлиньяж 1.5
	%\doublespacing % �?нтерлиньяж 2
	%\singlespacing % �?нтерлиньяж 1
	
	\usepackage{lastpage} % Узнать, сколько всего страниц в документе.
	
	\usepackage{soul} % Модификаторы начертания
	
	\usepackage{hyperref}
	\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}
	\hypersetup{				% Гиперссылки
		unicode=true,           % русские буквы в раздела PDF
		pdftitle={Заголовок},   % Заголовок
		pdfauthor={Автор},      % Автор
		pdfsubject={Тема},      % Тема
		pdfcreator={Создатель}, % Создатель
		pdfproducer={Производитель}, % Производитель
		pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова
		colorlinks=true,       	% false: ссылки в рамках; true: цветные ссылки
		linkcolor=red,          % внутренние ссылки
		citecolor=black,        % на библиографию
		filecolor=magenta,      % на файлы
		urlcolor=blue           % на URL
	}
	
	\usepackage{csquotes} % Еще инструменты для ссылок
	
	%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex}
	
	\usepackage{multicol} % Несколько колонок
	
	\usepackage{tikz} % Работа с графикой
	\usepackage{pgfplots}
	\usepackage{pgfplotstable}
	\usepackage{caption}
	\usepackage{subcaption}
	
	\usepackage{algorithm}
	\usepackage[noend]{algcompatible}
	
	\newcommand{\ba}{\mathbf{a}}
	\newcommand{\bb}{\mathbf{b}}
	\newcommand{\bw}{\mathbf{w}}
	\newcommand{\by}{\mathbf{y}}
	\newcommand{\bx}{\mathbf{x}}
	\newcommand{\bz}{\mathbf{z}}
	\newcommand{\cA}{\mathcal{A}}
	\newcommand{\bJ}{\mathbf{J}}
	\newcommand{\bQ}{\mathbf{Q}}
	\newcommand{\bbC}{\mathbb{C}}
	\newcommand{\bbR}{\mathbb{R}}
	\newcommand{\bbY}{\mathbb{Y}}
	\newcommand{\bW}{\mathbf{W}}
	\newcommand{\bH}{\mathbf{H}}
	\newcommand{\bF}{\mathbf{F}}
	\newcommand{\bR}{\mathbf{R}}
	\newcommand{\bX}{\mathbf{X}}
	
	\newcommand{\T}{{\text{\tiny\sffamily\upshape\mdseries T}}}
	\newcommand{\argmin}{\mathop{\arg \min}\limits}
	\newcommand{\argmax}{\mathop{\arg \max}\limits}
	
	
	\begin{document}
		
		\title
	{Quadratic programming optimization for Newton method}
	\date{}
	\maketitle
	\begin{center}
		R.\,V.~Isachenko,
		V.\,V.~Strijov
	\end{center}
	\textbf{Abstract:} 
	To optimize the model parameters the Newton method is widely used. 
	This method is second order optimization procedure that is unstable in real applications. 
	In this paper we propose the procedure to make the optimization process robust. 
	The idea is to select the set of model parameters which have to be optimized in the current step of optimization procedure.
	We show that in the case of nonlinear regression and logistic regression models the parameters selection could be performed by Quadratic Programming Feature Selection algorithm.
	It allows to find the set of independent parameters that are responsible for the residuals.
	We carried out the experiment to show how the proposed method works and compare it with other methods.
	
	The paper proposes the robust second-order optimization algorithm.
	The algorithm based on the iterative Newton method, which is unstable procedure.
	The authors suggest to select the set of active parameters in each optimization step.    The algorithm updates only parameters from this active set. 
	Quadratic programming feature selection is used to find the active set. It maximizes the relevance of model parameters to the residuals and minimizes the redundancy. Nonlinear regression and logistic regression models are investigated. 
	The proposed algorithm achieves the less error with comparison to the other methods.
	
	\bigskip
	\textbf{Keywords}: nonlinear regression, logistic regression, Newton method, quadratic programming feature selection
	
	\section*{Introduction}
	The error function models have a complex landscape with multiple local minima for models with extremely large number of parameters. 
	In this case the optimization algorithm brings to different solutions each time.
	
	The optimization algorithm is an iterative process. 
	In each step it updates the current point parameters to get the next approximation.
	There have been developed first-order optimization algorithms, which use the first derivatives of the error function. 
	The most famous algorithms are Gradient Descent, Nesterov Momentum~\cite{nesterov1983momentum}, Adagrad~\cite{duchi2011adagrad}, Adam~\cite{kingma2014adam}. 
	These algorithms are used for the deep neural networks optimization~\cite{goodfellow2016deeplearningbook}. 
	The Newton algorithm is the second-order algorithm, which uses the second derivatives of the error function. 
	It finds updates for the quadratic approximation of the error function and converges with the lesser number of iterations.
	The drawback of the second-order optimization methods is the huge and ill-conditioned Hessian matrix. 
	The optimization process in this case is computationally expensive and diverge. 
	\cite{avriel2003nonlinear,blaschke1997convergence}~propose the approximations for the Hessian matrix and regularization to overcome this problem .
	The paper~\cite{botev2017newtondeeplearning} applies the Newton method to the deep neural networks.
	
	This paper suggests to select the set of model parameters, which is optimized in each optimization step.
	The authors investigate the nonlinear regression model with the squared error function and the logistic regression model with the cross-entropy error function.
	For the nonlinear regression the Newton method and model linearization leads to the Gauss-Newton method. 
	Each step solves the linear regression problem. 
	The authors use the two-layer neural network as the nonlinear model. 
	The Newton method for logistic regression brings to Iteratively Least Squares~(IRLS) algorithm. 
	Here the optimization step is made in the direction given also by the linear regression problem solution.
	
	The paper proposes to apply the Quadratic Programming Feature Selection~(QPFS) algorithm~\cite{katrutsa2017comprehensive,rodriguez2010qpfs} to select the optimal set of model parameters. The QPFS algorithm selects features for the linear regression problem. We have the linear regression problem for both model in each step. The QPFS algorithm tries to maximize the relevances of features and minimize pairwise dependency between features~\cite{ding2005mrmr}. In our case it allows to find independent parameters which impact the model residuals the most.
	
	The computational experiment investigates a behaviour of the proposed algorithm near the optimal point and compared the algorithm with the other methods such as Gradient Descent, Nesterov Momentum, ADAM and Newton algorithms. 
	
		
	\section*{Problem Statement}

	The model $f( \bx, \bw), \bw \in \mathbb{R}^p$ predicts the target variable $y \in \bbY$, given the object $\bx \in \bbR^{n}$. The space $\bbY$ equals $\{0, 1\}$ for the binary classification problem and $\bbY = \bbR$ for the regression problem.
	There are given the design matrix~$\bX = [\bx_1, \dots, \bx_m]^{\T} \in \bbR^{m \times n}$ and the target vector~$\by = [y_1, \dots, y_m]^{\T} \in \bbY^{m}$. 
	The goal is to find the optimal parameters~$\bw^*$.
	The parameters~$\bw$ are fitted by the minimization of the error function:
	\begin{equation}
		\bw^* = \argmin_{\bw \in \bbR^p} S(\bw | \bX, \by, f).
		\label{eq:error_function}
	\end{equation}
	The investigated choices for the error function~$S(\bw | \bX, \by, f)$ are
	the squared error for the regression problem: 
		\begin{equation}
			S(\bw | \bX, \by, f) = \frac 12 \| \by - \mathbf{f}(\bX, \bw) \|_2^2 = \frac 12 \sum_{i=1}^m \bigl( y_i - f(\bx_i,  \bw)\bigr)^2,
			\label{eq:squared_error}
		\end{equation}
	 and the cross-entropy for the binary classification problem: 
		\begin{equation}
			S(\bw | \bX, \by, f) = \sum_{i=1}^m \bigl[y_i \log f (\bx_i , \bw) + (1-y_i) \log \bigl(1 - f (\bx_i , \bw)\bigr)\bigr].
			\label{eq:log_loss}
		\end{equation}
	
	The problem~\eqref{eq:error_function} is solved by iterative optimization procedures. 
	To obtain parameters in the step~$k$, the current parameters $\bw^{k-1}$ is updated by the rule
	\begin{equation}
		\bw^k = \bw^{k - 1} + \Delta \bw^{k - 1}.
		\label{eq:update_rule}
	\end{equation}
	This paper suggests to use the Newton optimization procedure to select parameters updates~$\Delta \bw$.
	
	The Newton method is unstable and computationally hard. Each iteration inverts the Hessian matrix $\bH = \nabla^2 S(\bw)$.
	This paper suggests the robust Newton algorithm. 
	Before the gradient step the authors propose to select the set of active model parameters, which have the greatest impact on the error function~$S(\bw)$.
	We update only the parameters with indices from a set $\cA \subseteq \{ 1, \dots, p \}$
	\begin{align*}
	\bw_{\cA}^k &= \bw_{\cA}^{k - 1} + \Delta \bw_{\cA}^{k - 1}, \quad \bw_{\cA} = \{w_j\}_{j \in \cA}; \\
	\bw_{\bar{\cA}}^k &= \bw_{\bar{\cA}}^{k - 1}, \quad \bw_{\bar{\cA}} = \{w_j\}_{j \notin \cA}.
	\end{align*}
	To select the set $\cA$ from all possible $2^p$ subsets we introduce the quality criteria 
	\begin{equation}
		\cA = \argmin_{\cA' \subseteq \{1, \dots, p\}} Q(\cA' | \bX, \by, f, \bw).
		\label{eq:subset_selection}
	\end{equation}
	The problem~\eqref{eq:subset_selection} is solved in each step $k$ of the optimization process for the current parameters~$\bw^k$.
	
	\section*{Newton method}
	
	The Newton method uses the first order optimization condition for problem~\eqref{eq:error_function} and linearize the gradient of $S(\bw)$
	\[
		\nabla S (\bw + \Delta \bw) = \nabla S(\bw) + \bH \cdot \Delta \bw = 0,
	\]
	\[
		\Delta \bw = - \bH^{-1} \nabla S(\bw).
	\]
	where $\bH = \nabla^2 S(\bw)$ is the Hessian matrix of the error function $S(\bw)$.
	
	The iteration~\eqref{eq:update_rule} of the Newton method is
	\[
		\bw^k = \bw^{k-1} - \bH^{-1} \nabla S(\bw).
	\]
	Each iteration inverts the Hessian matrix.
	The measure of ill-conditioning for the Heassian matrix $\bH$ is the condition number
	\[
		\kappa(\bH) = \frac{\lambda_{\text{max}}(\bH)}{\lambda_{\text{min}}(\bH)},
	\]
	where $\lambda_{\text{max}}(\bH), \lambda_{\text{min}}(\bH)$ are the maximum and minimum eigenvalues of~$\bH$. The large condition number~$\kappa(\bH)$ leads to instability of the optimization process.
	The proposed algorithm reduces the size of the Hessian matrix $\bH$ and makes the condition number $\kappa(\bH)$ smaller.
	The algorithm is implemented to the nonlinear regression problem and the binary classification problem.
	
	\section*{Nonlinear regression}
	Assume that the model $f(\bx , \bw)$ is close to linear in the neighborhood of the point $\bw + \Delta \bw$
	\[
	\mathbf{f}(\bX , \bw + \Delta \bw) \approx \mathbf{f}(\bX , \bw) + \bJ \cdot \Delta  \bw,
	\]
	
	where $\mathbf{J} \in \bbR^{m \times p}$ is the Jacobian matrix
	\begin{equation}
		\bJ = 
		\begin{pmatrix}
		\frac{\partial f(\bx_1 , \bw)}{\partial w_1} & \dots & 
		\frac{\partial f(\bx_1 , \bw)}{\partial w_p} \\
		\dots & \dots & \dots \\
		\frac{\partial f(\bx_m , \bw)}{\partial w_1} & \dots & 
		\frac{\partial f(\bx_m , \bw)}{\partial w_p}
		\end{pmatrix}.
	\end{equation}
	Under this assumption the gradient $\nabla S(\bw)$ and the Hessian matrix $\bH$ of the error function~\eqref{eq:squared_error} equal
	\begin{equation}
		\nabla S(\bw) = \bJ^{\T} (\by - \mathbf{f}), \quad \bH = \bJ^{\T} \bJ.
		\label{eq:nonlin_reg_deriv}
	\end{equation}
	It leads to the Gauss-Newton method and the update rule~\eqref{eq:update_rule} is 
	\[
		\bw_k = \bw_{k - 1} + (\bJ^{\T} \bJ)^{-1}\bJ^{\T}(\mathbf{f} - \by).
	\]
	The updates $\Delta \bw$ is the solution of the linear regression problem
	\begin{equation}
		\| \bz - \bF \Delta \bw \|_2^2 \rightarrow \min_{\Delta \bw \in \bbR^{p}},
		\label{eq:lin_reg_nonlin_reg}
	\end{equation}
	where $\bz = \mathbf{f} - \by$ and $\bF = \bJ$.
	
	We consider the feed-forward two layer neural network as the nonlinear model. In this case the model~$f(\bx, \bw)$ is given by
	\[
	f(\bx, \bw) = \sigma(\bx^{\T} \bW_1) \bw_2.
	\]
	Here~$\bW_1 \in \bbR^{n \times h}$ is the weight matrix which connects the input features with $h$ hidden units. The nonlinearity function $\sigma(\cdot)$ applied element-wise. The weights $\bw_2 \in \bbR^{h \times 1}$ connect the hidden units with the output. 
	The model parameters~$\bw$ is a concatenation of vectorized matrices~$\bW_1$, $\bw_2$.
	
	\section*{Logistic Regression}
	For logistic regression the model $f(\bx , \bw) = \sigma(\bx^{\T} \bw)$ with the sigmoid function~$\sigma(\cdot)$.
	The gradient and the Hessian of the error function~\eqref{eq:log_loss} equal
	\begin{equation}
		\nabla S(\bw) = \bX^{\T} (\mathbf{f} - \by), \quad \bH = \bX^{\T} \bR \bX,
		\label{eq:log_reg_deriv}
	\end{equation}
	where $\bR$ is a diagonal matrix with $f(\bx_i , \bw) \cdot (1 - f(\bx_i , \bw))$ diagonal entries.
	
	The update rule~\eqref{eq:update_rule} is
	\[
		\bw^k = \bw^{k - 1} + (\bX^{\T} \bR \bX)^{-1} \bX^{\T} (\by - \mathbf{f}).
	\]
	This algorithm is known as Iteralively Reweighted Least Squares (IRLS) algorithm. The updates $\Delta \bw$ is the solution of the linear regression problem
	\begin{equation}
		\| \bz - \bF \Delta \bw \|_2^2 \rightarrow \min_{\Delta \bw \in \bbR^{p}},
		\label{eq:lin_reg_log_reg}
	\end{equation}
where $\bz = \bR^{-1/2} (\by - \mathbf{f})$ and $\bF = \bR^{1/2}\bX$.
	
	\section*{Quadratic programming feature selection}
	If there are multicollinearity between columns of the design matrix~$\bX$ the solution of the linear regression problem
	\begin{equation}
	\| \by - \bX \bw\|_2^2 \rightarrow\min_{\bw \in \bbR^{n}}.
	\label{eq:linear_regression}
	\end{equation}
	is unstable. The feature selection methods finds the set $\cA \in \{1, \dots, n\}$ of representative columns in~$\bX$. 
	
	The goal of the QPFS is to select non-correlated features, which are relevant to the target vector $\by$.
	To formalise this approach let introduce two functions: Sim and Rel. 
	The Sim measures the redundancy between features, the Rel contains relevances between each feature and target vector. 
	We want to minimize the Sim function and maximize the Rel simultaneously.
	
	QPFS offers the explicit way to construct the functions Sim and Rel. 
	The method minimizes the following functional
	\begin{equation}
		\underbrace{\ba^{\T} \bQ \ba}_{\text{Sim}} - \alpha \cdot \underbrace{\vphantom{()} \mathbf{b}^{\T} \ba}_{\text{Rel}} \rightarrow \min_{\substack{\ba \geq 0 \\ \|\ba\|_1=1}}.
		\label{eq:quadratic_problem}
	\end{equation}
	The first term is associated with the Sim function and the second with the Rel. 
  	The matrix $\bQ \in \bbR^{n \times n}$ entries measure the pairwise similarities between features. 
  	The vector $\mathbf{b} \in \bbR^n$ expresses the similarities between each feature and the target vector~$\by$.
  	The normalized vector~$\ba$ shows the importance of each feature. 
  	The functional~\eqref{eq:quadratic_problem} penalizes the dependent features by the function Sim and encourages features relevant to the target by the function Rel. 
  	The parameter $\alpha$ allows to control the trade-off between the Sim and the Rel terms.
  	The authors of the original QPFS paper suggested the way to select $\alpha$ and make Sim and Rel terms impact are equal
  	\begin{equation*}
  		\alpha = \frac{\overline{\bQ}}{\overline{\bQ} + \overline{\bb}},
  	\end{equation*}
  	where $\overline{\bQ}$, $\overline{\bb}$ are the mean values of $\bQ$ and $\bb$ correspondingly.
  	Apply the thresholding for $\ba$ to find the optimal feature subset:
  	\[
  		j \in \mathcal{A} \Leftrightarrow a_j > \tau.
  	\]
  	
  	To measure similarity the authors use sample correlation coefficient between pairs of features for the Sim function and between features and target vector for the Rel function.
  	The problem~\eqref{eq:quadratic_problem} is convex if the matrix~$\bQ$ is positive semidefinite. In general it is not always true. 
  	To satisfy this condition we shift the matrix $\bQ$ spectrum and replace the matrix~$\bQ$ by $\bQ - \lambda_{\text{min}} \mathbf{I}$, where $\lambda_{\text{min}} $ is a $\bQ$ minimal eigenvalue.
  	
  	To make the optimization process of the Newton algorithm robust and stable we propose to implement the QPFS algorithm to the problems~\eqref{eq:lin_reg_nonlin_reg} and \eqref{eq:lin_reg_log_reg}. 
  	QPFS selects the set~$\cA$ of parameters updates~$\Delta \bw$, which have the greatest impact to the residuals and pairwise independent.
  	The functional~\eqref{eq:quadratic_problem} corresponds to the quality criteria~$Q(\cA | \bX, \by, f, \bw)$.
	\begin{equation}
		\cA = \argmin_{\cA' \subseteq \{1, \dots, p\}} Q(\cA' | \bX, \by, f, \bw) = \argmin_{\ba \geq 0, \, \|\ba\|_1=1} \bigl[\ba^{\T} \bQ \ba - \alpha \cdot \mathbf{b}^{\T} \ba \bigr].
	\end{equation}
  	
  	The sample correlation coefficient is equal to zero for the orthogonal vectors.
  	We show that in the optimal point $\bw^*$ the vector $\bz$ is orthogonal to the columns of the matrix $\bF$ for the considered problems. 
  	It leads to the QPFS vector $\bb = \boldsymbol{0}$.
  	First order optimization condition guarantees this property for the nonlinear regression problem
	\[
		\bF^{\T} \bz = \bJ^{\T} (\mathbf{f} - \by) = - \nabla S(\bw^*) = \boldsymbol{0},
	\]
	and the logistic regression problem
	\[
		\bF^{\T} \bz = \bX \bR^{-1/2} \bR^{1/2} (\by - \mathbf{f}) = \bX^{\T} (\by - \mathbf{f}) = \nabla S(\bw^*) = \boldsymbol{0}.
	\]

	\section*{Step size}
	
	The step size of the Newton method could be excessively large. To control the step size of the weight updates we add the parameter $\eta$ in the update rule~\eqref{eq:update_rule}
	\[
		\bw^k = \bw^{k - 1} + \eta \Delta \bw^{k - 1}, \quad \eta \in [0, 1].
	\]
	To select the appropriate step size $\eta$ the Armijo rule is used. We choose $\eta$ as large as possible to satisfy the following condition
	\[
		S(\bw^{k - 1} + \eta \Delta \bw^{k - 1}) < S(\bw^{k - 1}) + \gamma \eta \nabla S^{\T}(\bw^{k-1})\bw^{k - 1}, \quad \gamma \in [0, 0.5].
	\]
	
  	\section*{Experiment}
  	The goal of the computational experiment is to explore properties of the proposed algorithm and compare it with the other methods. The pseudocode of the algorithm is given in the algorithm~\eqref{pc:QPFSNewton}.
  	
  	
  	\begin{algorithm}
  		\caption{QPFS + Newton algorithm}
  		\label{pc:QPFSNewton}
  		\begin{algorithmic}
  			\REQUIRE $\varepsilon$~-- tolerance;\\
  			\hspace{1.53cm}$\tau$~-- QPFS solution threshold;\\
  			\hspace{1.53cm}$\gamma$~-- Armijo rule parameter.
  			\ENSURE $\bw^*$;
  			\STATE  initialize $\bw^0$;
  			\STATE $k := 1$;
  			\REPEAT
  			\STATE compute $\bz$ and $\bF$ for~\eqref{eq:lin_reg_nonlin_reg} or~\eqref{eq:lin_reg_log_reg} ;
  			\vspace{0.1cm}
  			\STATE $\bQ := \text{Sim} (\bF)$, $\bb := \text{Rel}(\bF, \bz)$, $\alpha = \frac{\overline{\bQ}}{\overline{\bQ} + \overline{\bb}}$;
  			\vspace{0.1cm}
  			\STATE $\ba := \argmin_{\ba \geq 0, \, \|\ba\|_1=1}\ba^{\T} \bQ \ba - \alpha \cdot \mathbf{b}^{\T} \ba$;
  			\vspace{0.1cm}
  			\STATE $\cA = \{j \mid a_j > \tau\}$;
  			\vspace{0.1cm}
  			\STATE compute $\nabla S(\bw_{k-1})$, $\bH$ from \eqref{eq:nonlin_reg_deriv} or \eqref{eq:log_reg_deriv};
  			\vspace{0.1cm}
  			\STATE $\Delta \bw^{k-1} = - \bH^{-1} \nabla S(\bw^{k-1})$;
  			\vspace{0.1cm}
  			\STATE $\eta := \text{ArmijoRule}(\bw_{k-1}, \gamma)$;
  			\vspace{0.1cm}
  			\STATE $\bw_{\cA}^k = \bw_{\cA}^{k - 1} + \eta \Delta \bw_{\cA}^{k - 1}$;
  			\vspace{0.1cm}
  			\STATE $k := k + 1$;
  			\vspace{0.1cm}
  			\UNTIL{$\frac{\| \bw^k - \bw^{k-1} \|}{\| \bw^k \|} < \varepsilon$}
  		\end{algorithmic}
  	\end{algorithm}
  	
  	We investigate the dependence of the QPFS parameters for the problems~\eqref{eq:lin_reg_nonlin_reg},~\eqref{eq:lin_reg_log_reg}. 
  	Assume that the parameter vector~$\bw^0$ lies near the optimal parameters vector~$\bw^*$. 
  	We consider the line segment
  	\[
  	\bw_{\beta} = \beta \bw^* + (1 - \beta) \bw^0; \, \beta \in [0, 1] .
  	\]
  	
  	We generate the dataset with 300 samples and 7 features for the logistic regression problem. 
  	The landscape of the error function~\eqref{eq:log_loss} on the two random selected parameters grid is shown in the figure~\ref{fig:log_reg_error}.
  	The surface is convex with stretched level lines along some model parameters.
  	Add the random noise to the optimum parameters~$\bw^*$ to get the point~$\bw^0$. The behaviour of the Rel term vector~$\bb$ on the line segment between~$\bw^0$ and~$\bw^*$ is illustrated in the figure~\ref{fig:log_reg_b_wrt_beta}.
  	The components of the vector~$\bb$ starts to decrease sharply nearing the optimal point.
	\begin{figure}[!h]
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figs/log_reg_error}
			\caption{Error function landscape}
			\label{fig:log_reg_error}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=\linewidth]{figs/log_reg_b_wrt_beta.eps}
			\caption{Feature relevances}
			\label{fig:log_reg_b_wrt_beta}
		\end{subfigure}
		\caption{Logistic regression}
	\end{figure}
  	
  	For nonlinear regression model we used the classical Boston Housing dataset with 506 objects and 13 features.
  	The neural network contains two hidden neurons for simplicity.
  	The error function landscape for the neural network model is more complex. 
  	It is not convex and could contain multiple local minimum.
  	The two-dimensional error function landscape for this dataset is shown in the figure~\ref{fig:neural_error}. 
  	The grid is obtained by selecting two random weights from the matrix~$\bW_1$.
  	We use the same strategy to investigate how the linear term vector~$\bb$ is changing moving from~$\bw^0$ to~$\bw^*$. 
  	The result is shown in the figure~\ref{fig:neural_b_wrt_beta}.
  	The vector $\bb$ components decline near optimum. Reaching the optimum the different weights influence on the model residuals~$\bz$.
  
  \begin{figure}[!h]
  	\centering
  	\begin{subfigure}{.5\textwidth}
  		\centering
  		\includegraphics[width=\linewidth]{figs/neural_error.eps}
  		\caption{Error function landscape}
  		\label{fig:neural_error}
  	\end{subfigure}%
  	\begin{subfigure}{.5\textwidth}
  		\centering
  		\includegraphics[width=\linewidth]{figs/neural_b_wrt_beta.eps}
  		\caption{Feature relevances}
  		\label{fig:neural_b_wrt_beta}
  	\end{subfigure}
  	\caption{Neural network, first layer}
  \end{figure}

	Figure~\ref{fig:irls_qpfs_2d} shows the optimization process for the proposed procedure in the case of logistic regression with two model parameters. 
	Even for two-dimensional problem the solution of Newton method is unstable and the condition number of Hessian matrix~$\bH$ is could be extremely large. 
	In each step of the algorithm the QPFS procedure selects the parameters that should be optimized. In this example the proposed algorithm selects and updates only one parameter per iteration in the first steps. It makes the algorithm more robust.
	
	\begin{figure}[!h]
		\centering
		\includegraphics[width=0.6\linewidth]{figs/irls_qpfs_2d.eps}	 
		\caption{Optimization process for logistic regression with QPFS+Newton algorithm}
		\label{fig:irls_qpfs_2d}
	\end{figure}

	Figure~\ref{fig:active_params_wrt_iters} shows the sets of active parameters over iterations for Boston housing dataset and neural network with two 2 hidden neurons. The dark cells correspond to the active parameters that we optimize.

	\begin{figure}[!h]
		\centering
		\includegraphics[width=\linewidth]{figs/active_params_wrt_iters.eps}	 
		\caption{Active parameters sets over optimization process}
		\label{fig:active_params_wrt_iters}
	\end{figure}
	
	In the considered examples the condition number~$\kappa (\bH)$ of the original Newton number in some iterations was extremely large. 
	The selection of the active parameters allowed to reduce the condition number significantly. 
	
	We compared the proposed algorithm with the existing methods, namely Gradient Descent~(GD), Nesterov Momentum, ADAM and the original Newton algorithm. 
	Experiments were carried out for nonlinear regression problem. 
	The datasets were chosen from the UCI achive repository~\cite{uci2017}. 
	The results are shown in the table~\ref{tbl:nonlin_reg_results}. 
	For each dataset there are two rows which contains mean squared error for the train~(first row) and test~(second row) data. 
	We used 5-fold cross validation to find the average error and its standard deviation. 
	The proposed algorithm shows the least error among three of four datasets. 
	The generalization of the proposed algorithm is better than the generalization of the Newton method. 
	It shows the difference between training and test error.
	\begin{table}[!h]
		\footnotesize{
		\caption{Train and test mean squared errors for all datasets and algorithms}
		\label{tbl:nonlin_reg_results}
		\centering
		\begin{tabular}{|l|c|c|c|c|c|c|}
			\hline
			Datasets & \begin{tabular}[c]{@{}c@{}}\#obj\\ \#feat\end{tabular} 
			& GD 
			& Nesterov 
			& ADAM 
			& Newton 
			&
			\begin{tabular}[c]{@{}c@{}}QPFS+Newton\\ (proposed)\end{tabular} \\ 
			\hline
			Boston House
			& 506
			& $27.2\pm4.6$
			& $46.0\pm11.0$
			& $35.4\pm2.5$           
			& $22.1\pm15.2$            
			& $20.9\pm10.4$   \\  
			Prices
			&\multicolumn{1}{c|}{13}
			& \multicolumn{1}{c|}{$32.4\pm5.6$}
			& \multicolumn{1}{c|}{$53.3\pm11.5$}
			& \multicolumn{1}{c|}{$37.8\pm7.0$}
			& \multicolumn{1}{c|}{$28.9\pm13.6$}
			& \multicolumn{1}{c|}{$\mathbf{24.5\pm9.4}$}\\ 
			\hline
			Communities
			& 1994
			& $48.0\pm6.4$
			& $31.4\pm2.8$
			& $23.3\pm3.7$        
			& $18.3\pm3.4$          
			&  $26.7\pm3.1$  \\ 
			and Crime
			&\multicolumn{1}{c|}{99}
			& \multicolumn{1}{c|}{$47,5\pm6.5$}
			& \multicolumn{1}{c|}{$32.9\pm4.3$}
			& \multicolumn{1}{c|}{$28,1\pm4.5$}
			& \multicolumn{1}{c|}{$28.8\pm3.6$}
			& \multicolumn{1}{c|}{$\mathbf{28.4\pm3.0}$} \\ 
			\hline
			Forest
			& 517
			& $18.9\pm0.4$
			& $1.83\pm0.4$
			& $1.81\pm0.6$             
			& $17.7\pm0.4$             
			& $17.9\pm0.4$   \\ 
			Fires
			&\multicolumn{1}{c|}{10}
			& \multicolumn{1}{c|}{$\mathbf{20.0\pm2.1}$}
			& \multicolumn{1}{c|}{ $20.2\pm2.2$}
			& \multicolumn{1}{c|}{ $\mathbf{20.0\pm2.0}$}
			& \multicolumn{1}{c|}{ $20.6\pm1.4$}
			& \multicolumn{1}{c|}{ $20.2\pm2.2$} \\ 
			\hline
			Residential
			& 372
			&  $51.6\pm17.7$
			&  $32.6\pm19.5$
			&  $30.0\pm24.8$            
			&  $35.5\pm24.7$            
			&   $30.3\pm10.7$ \\ 
			Building
			&\multicolumn{1}{c|}{103}
			& \multicolumn{1}{c|}{ $53.7\pm13.9$}
			& \multicolumn{1}{c|}{ $34.1\pm13.6$}
			& \multicolumn{1}{c|}{ $34.1\pm19.4$}
			& \multicolumn{1}{c|}{ $35.0\pm15.6$}
			& \multicolumn{1}{c|}{ $\mathbf{30.9\pm5.3}$} \\ 
			\hline
		\end{tabular}
	}
	\end{table}
	\section*{Conclusion}
	The paper suggests the new approach to the second-order optimization. 
	The selection of the set of active model parameters allows to make steps in the directions more relevant to the residuals.
	The experiments show the proposed algorithm improves the generalization ability and reduces the model error.
  
  \bibliographystyle{unsrt}
  \bibliography{papers_qpfs.bib}
	\end{document}